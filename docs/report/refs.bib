@misc{main_paper,
	title = {A {General} {Framework} for {Jersey} {Number} {Recognition} in {Sports} {Video}},
	url = {http://arxiv.org/abs/2405.13896},
	doi = {10.48550/arXiv.2405.13896},
	abstract = {Jersey number recognition is an important task in sports video analysis, partly due to its importance for long-term player tracking. It can be viewed as a variant of scene text recognition. However, there is a lack of published attempts to apply scene text recognition models on jersey number data. Here we introduce a novel public jersey number recognition dataset for hockey and study how scene text recognition methods can be adapted to this problem. We address issues of occlusions and assess the degree to which training on one sport (hockey) can be generalized to another (soccer). For the latter, we also consider how jersey number recognition at the single-image level can be aggregated across frames to yield tracklet-level jersey number labels. We demonstrate high performance on image- and tracklet-level tasks, achieving 91.4\% accuracy for hockey images and 87.4\% for soccer tracklets. Code, models, and data are available at https://github.com/mkoshkina/jersey-number-pipeline.},
	urldate = {2025-02-28},
	publisher = {arXiv},
	author = {Koshkina, Maria and Elder, James H.},
	month = may,
	year = {2024},
	note = {arXiv:2405.13896 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Riley\\Zotero\\storage\\C4N6R6IX\\Koshkina and Elder - 2024 - A General Framework for Jersey Number Recognition in Sports Video.pdf:application/pdf;Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\PVMQQTD5\\2405.html:text/html},
}
@article{bhargavi,
	title = {Jersey number detection using synthetic data in a low-data regime},
	volume = {5},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.988113/full},
	doi = {10.3389/frai.2022.988113},
	abstract = {Player identification is an essential and complex task in sports video analysis. Different strategies have been devised over the years and identification based on jersey numbers is one of the most common approaches given its versatility and relative simplicity. However, automatic detection of jersey numbers is challenging due to changing camera angles, low video resolution, small object size in wide-range shots, and transient changes in the player's posture and movement. In this paper, we present a novel approach for jersey number identification in a small, highly imbalanced dataset from the Seattle Seahawks practice videos. We generate novel synthetic datasets of different complexities to mitigate the data imbalance and scarcity in the samples. To show the effectiveness of our synthetic data generation, we use a multi-step strategy that enforces attention to a particular region of interest (player's torso), to identify jersey numbers. The solution first identifies and crops players in a frame using a person detection model, then utilizes a human pose estimation model to localize jersey numbers in the detected players, obviating the need for annotating bounding boxes for number detection. We experimented with two sets of Convolutional Neural Networks (CNNs) with different learning objectives: multi-class for two-digit number identification and multi-label for digit-wise detection to compare performance. Our experiments indicate that our novel synthetic data generation method improves the accuracy of various CNN models by 9\% overall, and 18\% on low frequency numbers.},
	language = {English},
	urldate = {2025-04-09},
	journal = {Frontiers in Artificial Intelligence},
	author = {Bhargavi, Divya and Gholami, Sia and Pelaez Coyotl, Erika},
	month = oct,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {CNN - convolutional neural network, Ensemble model, Jersey number detection, low-data regime, synthetic data},
	file = {Full Text PDF:C\:\\Users\\Riley\\Zotero\\storage\\PHLDC7U8\\Bhargavi et al. - 2022 - Jersey number detection using synthetic data in a low-data regime.pdf:application/pdf},
}

@misc{soccernet_repo,
	title = {{SoccerNet}/sn-jersey},
	url = {https://github.com/SoccerNet/sn-jersey},
	abstract = {Repository containing all necessary codes to get started on the SoccerNet Jersey Number Recognition challenge.},
	urldate = {2025-04-09},
	publisher = {SoccerNet},
	month = mar,
	year = {2025},
	note = {original-date: 2023-02-04T14:00:03Z},
}

@misc{clipter,
	title = {{CLIPTER}: {Looking} at the {Bigger} {Picture} in {Scene} {Text} {Recognition}},
	shorttitle = {{CLIPTER}},
	url = {http://arxiv.org/abs/2301.07464},
	doi = {10.48550/arXiv.2301.07464},
	abstract = {Reading text in real-world scenarios often requires understanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they operate on cropped text images. In this study, we harness the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component gradually shifts to the context-enhanced representation, allowing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art results across multiple benchmarks. Furthermore, our analysis highlights improved robustness to out-of-vocabulary words and enhanced generalization in low-data regimes.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Aberdam, Aviad and Bensa√Ød, David and Golts, Alona and Ganz, Roy and Nuriel, Oren and Tichauer, Royee and Mazor, Shai and Litman, Ron},
	month = jul,
	year = {2023},
	note = {arXiv:2301.07464 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Riley\\Zotero\\storage\\5TFGLFAE\\Aberdam et al. - 2023 - CLIPTER Looking at the Bigger Picture in Scene Text Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\Y39JSZBT\\2301.html:text/html},
}

@misc{original_pipeline_repo,
	title = {mkoshkina/jersey-number-pipeline},
	url = {https://github.com/mkoshkina/jersey-number-pipeline/tree/main},
	urldate = {2025-04-09},
	author = {Koshkina, Maria},
	file = {mkoshkina/jersey-number-pipeline\: A General Framework for Jersey Number Recognition in Sports Video:C\:\\Users\\Riley\\Zotero\\storage\\QIAAYQE7\\main.html:text/html},
}

@misc{parseq,
	title = {baudm/parseq},
	copyright = {Apache-2.0},
	url = {https://github.com/baudm/parseq},
	abstract = {Scene Text Recognition with Permuted Autoregressive Sequence Models (ECCV 2022)},
	urldate = {2025-04-09},
	author = {Bautista, Darwin},
	month = apr,
	year = {2025},
	note = {original-date: 2021-11-24T02:48:36Z},
	keywords = {computer-vision, eccv, eccv2022, ocr, optical-character-recognition, scene-text-recognition, text-recognition, vision-transformer},
}

@misc{zhao_decoder,
	title = {Decoder {Pre}-{Training} with only {Text} for {Scene} {Text} {Recognition}},
	url = {http://arxiv.org/abs/2408.05706},
	doi = {10.48550/arXiv.2408.05706},
	abstract = {Scene text recognition (STR) pre-training methods have achieved remarkable progress, primarily relying on synthetic datasets. However, the domain gap between synthetic and real images poses a challenge in acquiring feature representations that align well with images on real scenes, thereby limiting the performance of these methods. We note that vision-language models like CLIP, pre-trained on extensive real image-text pairs, effectively align images and text in a unified embedding space, suggesting the potential to derive the representations of real images from text alone. Building upon this premise, we introduce a novel method named Decoder Pre-training with only text for STR (DPTR). DPTR treats text embeddings produced by the CLIP text encoder as pseudo visual embeddings and uses them to pre-train the decoder. An Offline Randomized Perturbation (ORP) strategy is introduced. It enriches the diversity of text embeddings by incorporating natural image embeddings extracted from the CLIP image encoder, effectively directing the decoder to acquire the potential representations of real images. In addition, we introduce a Feature Merge Unit (FMU) that guides the extracted visual embeddings focusing on the character foreground within the text image, thereby enabling the pre-trained decoder to work more efficiently and accurately. Extensive experiments across various STR decoders and language recognition tasks underscore the broad applicability and remarkable performance of DPTR, providing a novel insight for STR pre-training. Code is available at https://github.com/Topdu/OpenOCR},
	urldate = {2025-04-09},
	publisher = {arXiv},
	author = {Zhao, Shuai and Du, Yongkun and Chen, Zhineng and Jiang, Yu-Gang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05706 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Riley\\Zotero\\storage\\Q3MLB3GN\\Zhao et al. - 2024 - Decoder Pre-Training with only Text for Scene Text Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\5IUZRCAE\\2408.html:text/html},
}
